# Vision Transformer from Scratch ğŸ‘¶
ê¸°ê°„ : 2023ë…„ 9ì›” ~ 11ì›” <br>
ì°¸ê³  ë¬¸í—Œ : [An Image Is Worth 16 X 16 Words:Transformers For Image Recognition At Scale](https://arxiv.org/abs/2010.11929) <br>
í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” Transformer ëª¨ë¸ì„ ë¹ ë¥´ê³  ê°„ë‹¨í•˜ê²Œ ì´í•´í•˜ê³ , êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. <Br>

## Vision Transformer
Vision TransformerëŠ” ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•´ Transformer ì•„í‚¤í…ì²˜ë¥¼ ì ìš©í•œ ëª¨ë¸ë¡œ, í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤. <br>

| Input Data ìƒì„± ê³¼ì • | Transformer Encoder ê³¼ì • | 
|:---:|:---:|
|1. ì´ë¯¸ì§€ ë¶„í• <Br>(Image Patching) | 1. ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜<br>(Multi-Head Self-Attention) |
|2. íŒ¨ì¹˜ ì¸ì½”ë”©<br>(Patch Embedding) | 2. í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬<br>(Feedforwaed Network) |
|3. ìœ„ì¹˜ ì„ë² ë”©<br>(Position Embedding) | 3. ë ˆì´ì–´ ì •ê·œí™” ë° ì”ì°¨ ì—°ê²°<br>(Layer Normalization and Residual Connections) |
|4. í´ë˜ìŠ¤ í† í° ì¶”ê°€<Br>(Class token) | |



### 1. Iunput Data ìƒì„±ê³¼ì •

![Input](https://github.com/kingodjerry/vision_transformer/assets/143167244/d345a51c-e3a9-4ee8-a3be-4608e672a24b)

í•´ë‹¹ ë‹¨ê³„ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ Transformerê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤. <br>
<br> 
**1. ì´ë¯¸ì§€ ë¶„í• (Image Patching)** <br>
ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€(Patch)ë¡œ ë¶„í• í•œë‹¤. 224X224 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ 16X16 í¬ê¸°ì˜ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ë©´ ì´ 196ê°œì˜ íŒ¨ì¹˜ê°€ ìƒì„±ëœë‹¤. <br>
**2. íŒ¨ì¹˜ ì¸ì½”ë”©(Patch Embedding)** <br> 
ê° íŒ¨ì¹˜ë¥¼ 1ì°¨ì› ë°±í„°ë¡œ ë³€í™˜í•œë‹¤. íŒ¨ì¹˜ë¥¼ í¼ì³ì„œ 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“¤ê³ , ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ì—¬ ê³ ì •ëœ í¬ê¸°ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤. 16X16X3(RGB) íŒ¨ì¹˜ëŠ” 768ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ëœë‹¤. <br>
**3. ìœ„ì¹˜ ì„ë² ë”©(Position Embedding)** <br> 
ìˆœì„œ ì •ë³´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ê° íŒ¨ì¹˜ ì„ë² ë”©ì— ì •ë³´ë¥¼ ë”í•´ì¤€ë‹¤. ì´ëŠ” Transformerì˜ self-attention ë§¤ì»¤ë‹ˆì¦˜ì´ íŒ¨ì¹˜ ê°„ì˜ ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•¨ì´ë‹¤. <br>
**4. í´ë˜ìŠ¤ í† í°(Class token)** <br> 
ë¶„ë¥˜ë¥¼ ìœ„í•´ íŠ¹ë³„í•œ í´ë˜ìŠ¤ í† í°ì„ ì¶”ê°€í•œë‹¤. í´ë˜ìŠ¤ í† í°ì€ í•™ìŠµê³¼ì •ì—ì„œ ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ ìš”ì•½í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì´ í† í°ì€ ë‹¤ë¥¸ íŒ¨ì¹˜ ì„ë² ë”©ê³¼ í•¨ê»˜ Transformerì— ì…ë ¥ëœë‹¤. <br> 

### 2. Transformer Encoder ê³¼ì •

![Encoder](https://github.com/kingodjerry/vision_transformer/assets/143167244/86bb3dae-5d37-4750-9bc5-735b10e7ff8f)

í•´ë‹¹ ë‹¨ê³„ì—ì„œëŠ” ì• ê³¼ì •ì—ì„œ ìƒì„±ëœ ì…ë ¥ ë°ì´í„°ë¥¼ Transformer Encoderë¥¼ í†µí•´ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì´ë‹¤. <br>
<br>
**1. ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜(Multi-Head Self-Attention)** <br>
ê° íŒ¨ì¹˜ì™€ í´ë˜ìŠ¤ í† í°ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•œë‹¤. <br>
**2. í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(Feedforward Network)** <br> 
ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë” ê¹Šì´ í•™ìŠµí•˜ê¸° ìœ„í•´ ê° í† í° ì„ë² ë”©ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ë‘ ê°œì˜ ì™„ì „ ì—°ê²°ì¸µì„ í¬í•¨í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ë¡œ GELUê°€ ì‚¬ìš©ëœë‹¤.  <br>
**3. ë ˆì´ì–´ ì •ê·œí™” ë° ì”ì°¨ ì—°ê²°(Layer Normalization and Residual Connections)** <br> 
ê° ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜ê³¼ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ëª¨ë“ˆì˜ ì¶œë ¥ì—ëŠ” ë ˆì´ì–´ ì •ê·œí™”ì™€ ì”ì°¨ ì—°ê²°ì´ ì ìš©ëœë‹¤. ì´ ê³¼ì •ì€ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³ , ì •ë³´ íë¦„ì„ ì›í™œí•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. <br>

### 3. ìµœì¢… ì¶œë ¥
Transformer Encoderì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì—ì„œ **í´ë˜ìŠ¤ í† í°**ì„ ì¶”ì¶œí•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œ ë¶„ë¥˜ í—¤ë“œ(ì£¼ë¡œ ì„ í˜• ë¶„ë¥˜ê¸°)ë¥¼ í†µí•´ ì´ë¯¸ì§€ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤. <br>


# ViT Fine-Tuning ğŸŒ¾
ì•ì„œ ì´í•´í•œ Vision Transformer ëª¨ë¸ì„ Fine-tuningí•˜ì—¬ **ê±´ê°•í•œ ì½©ì**ê³¼ **í•´ë¡œìš´ ì½©ì**ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. 

### Pre-train model
Pre-train model : vit-base-patch-224-in21k model (êµ¬ê¸€ ì œê³µ)

### Dataset
Datasetì€ datasets transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ 'beans' ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. <br>
í•´ë‹¹ ë°ì´í„°ì…‹ì€ ì½©ì ì´ë¯¸ì§€ ë°ì´í„°ë¡œ, ë³‘ì— ê±¸ë¦° ì½©ìê³¼ ê±´ê°•í•œ ì½©ììœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆëŠ” ë°ì´í„° ì„¸íŠ¸ì´ë‹¤. <br>
Train : 1034, Validation : 133, Test : 128ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, Labelì€ 'angular_leaf_spot', 'bean_rust', 'healthy'ë¡œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤. <br> 

**1. transformerì™€ dataset ë‹¤ìš´ë¡œë“œ**
```
   pip install datasets transformers
   pip install transformers[torch]

   from datasets import load_dataset

   dataset = load_dataset('beans')
```
**2. dataset í™•ì¸** (ê° í´ë˜ìŠ¤ì˜ ì˜ˆì œ) <br>
**3. ViT ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ* Load** <br>
  *ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ : ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì „ì²˜ë¦¬í•˜ëŠ” ì—­í•  ìˆ˜í–‰(ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •, íŒ¨ì¹˜ ìƒì„±, ì •ê·œí™”, í…ì„œ ë³€í™˜ ë“±ë“±) <br>
**4. Input dataë¡œ ë³€í™˜** - tensorë¡œ ë³€í™˜ <br>
**5. Pretrain model Load** <br>
```
from transformers import ViTForImageClassification

labels = ds['train'].features['labels'].names

model = ViTForImageClassification.from_pretrained(
    model_name_or_path,
    num_labels=len(labels),
    id2label={str(i): c for i, c in enumerate(labels)},
    label2id={c: str(i) for i, c in enumerate(labels)}
)
```
**6. íŒŒë¼ë¯¸í„° ì •ì˜**
```
from transformers import ViTForImageClassification

labels = ds['train'].features['labels'].names

model = ViTForImageClassification.from_pretrained(
    model_name_or_path,
    num_labels=len(labels),
    id2label={str(i): c for i, c in enumerate(labels)},
    label2id={c: str(i) for i, c in enumerate(labels)}
)
```
**7. í•™ìŠµ ì¤€ë¹„**
```
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    compute_metrics=compute_metrics,
    train_dataset=prepared_ds["train"],
    eval_dataset=prepared_ds["validation"],
    tokenizer=processor,
)
```
**8. í•™ìŠµ**
```
train_results = trainer.train()
trainer.save_model()
trainer.log_metrics("train", train_results.metrics)
trainer.save_metrics("train", train_results.metrics)
trainer.save_state()
```
**9. í•™ìŠµ í‰ê°€**
```
metrics = trainer.evaluate(prepared_ds['validation'])
trainer.log_metrics("eval", metrics)
trainer.save_metrics("eval", metrics)
```
   

## Reference
```vision_transformer.ipynb```ì˜ ì½”ë“œëŠ” [tintnë‹˜ì˜ vision-transformer-from-scratch](https://github.com/tintn/vision-transformer-from-scratch)ì—ì„œ cloneí•˜ì˜€ìŠµë‹ˆë‹¤. <br>
[ì°¸ê³  ë¸”ë¡œê·¸ 1 - Implementing Vision Transformer (ViT) from Scratch / Tin Nguyen ](https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0) <br>
[ì°¸ê³  ë¸”ë¡œê·¸ 2 - [AI/ViT] Vision Transformer(ViT), ê·¸ë¦¼ìœ¼ë¡œ ì‰½ê²Œ ì´í•´í•˜ê¸° / ë¯¸ìŠˆë‹ˆ ](https://mishuni.tistory.com/137) <br>
